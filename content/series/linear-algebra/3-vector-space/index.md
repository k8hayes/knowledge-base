---
title: "Vector Space"
date: 2020-08-31T13:13:34-04:00
summary: "We introduce some basic terminology - vector space, subspace, span, basis, dimension, norm and distance. These concepts lay the foundation for future discussions on matrices and matrix properties." # appears in list of posts
categories: ["Linear Algebra"] # main category; shown in post metadata
tags: ["Linear Algebra", "Matrix", "Operations", "Visualization"] # list of related tags

slug: "linear-algebra-vector-space"
toc: true # table of contents button in post

# featured posts are shown on the homepage
featured: false
draft: false

weight: 30 # smaller values are listed first

# full-width featured image
# To use, add an image named `featured.jpg/png` to your page's folder, or
# fill the unsplash_id and the photo will be automatically retrieved.
header_image:
    caption: "Ocean clouds seen from space." # Give credits here, or whatever captions you want to add (support markdown)
    unsplash_id: "yZygONrUBe8" # Unsplash ID of the picture
---

Associated with any matrix is a very important characteristic called the rank. There are several ways
of defining the rank, and the most fundamental of these is in terms of the dimension of a linear space.

## Vector space

A non-empty set of vectors $V$ is called a `vector space` (or linear space) if:

1. For all $\boldsymbol{x}, \boldsymbol{y} \in V$, $\boldsymbol{x} + \boldsymbol{y} \in V$, i.e. $V$ is closed under addition.
2. For all $\boldsymbol{x} \in V$, $k \in \mathbb{R}$, $k\boldsymbol{x} \in V$, i.e. $V$ is closed under scalar multiplication.

The second condition implies that the zero vector is always included in any vector space $V$. Alternatively, we may say that $V$ is closed under linear combination, i.e.

$$
k_1 \boldsymbol{x}_1 + \cdots + k_n \boldsymbol{x}_n \in V
$$

for all $\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n \in V$ and $k_1, \cdots, k_n \in \mathbb{R}$.

For example, below are some valid/invalid vector spaces:

1. $V = \\{(x, y)^\prime: x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space.
2. $V = \\{(x, y)^\prime: x > y\\}$ is not a vector space.
3. $V = \\{(x, y, 0)^\prime: x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space.
4. $V = \\{(x, y, z)^\prime: 2x = y, x \in \mathbb{R}, y \in \mathbb{R}, z \in \mathbb{R}\\}$.

## Subspace

A set $W$ is a `subspace` of a vector space $V$ if $W \subset V$, and $W$ itself is a vector space.

For example, $V = \mathbb{R}^2 = \\{(x, y)^\prime \mid x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space. Let

$$
W_1 = \\{ (x, y)^\prime: y-2x = 0, x, y \in \mathbb{R} \\}
$$

We first check if $W_1$ is a subset of $V$. For any element $\boldsymbol{u} = (u_1, u_2)^\prime \in W_1$ such that $u_2 = 2u_1$, as $\boldsymbol{u}$ is a $2 \times 1$ vector, $\boldsymbol{u} \in V = \mathbb{R}^2$.

Then we need to show that $W_1$ itself is a vector space by showing that $W_1$ is closed under linear combination. For all $\boldsymbol{u}, \boldsymbol{v} \in W_1$, $\boldsymbol{u} = (u_1, u_2)^\prime$ such that $u_2 = 2u_1$, $\boldsymbol{v} = (v_1, v_2)^\prime$ such that $v_2 = 2v_1$, consider

$$
a \boldsymbol{u} + b \boldsymbol{v} = (au_1 + bv_1, au_2 + bv_2)^\prime \in W_1 \quad \forall a, b.
$$

We need to show that $2(au_1 + bv_1) = au_2 + bv_2$, which is obvious. Therefore $W_1$ is a subspace.

Let

$$
W_2 = \\{ (x, y)^\prime: y - 2x + 1 = 0 \\}
$$

Following the same procedure, we can show that $W_2$ is not closed under addition and therefore is not a subspace.

## Span

Let $V$ be an ambient vector space ($\mathbb{R}^2, \mathbb{R}^3, \cdots$). Let $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$[^v-and-s] where $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m \in V$. The `(linear) span` of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$, or $S$, is the set that contains all linear combinations of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. The span is denoted:

$$
\begin{aligned}
    W &= \mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) = span(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) \\\\
    &= \left\\{ \boldsymbol{u} \mid \boldsymbol{u} = \sum\_{i=1}^m k_i \boldsymbol{u}_i, k_i \in \mathbb{R} \right\\}
\end{aligned}
$$

[^v-and-s]: The difference between $V$ and $S$ is that $S$ is a finite set.

We say $W$ is spanned or generated by $S$, or by $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. For example,

$$
\boldsymbol{u}_1 = (1, 0, 0)^\prime, \quad \boldsymbol{u}_2 = (0, 1, 1)^\prime
$$

$W = \mathcal{L}(\boldsymbol{u}_1, \boldsymbol{u}_2)$ contains all possible linear combinations of $\boldsymbol{u}_1$ and $\boldsymbol{u}_2$, such as $(0, 0, 0)^\prime$, $(1, 2, 2)^\prime$, etc.

**A span is a subspace.** Let $W = \mathcal{L}(\boldsymbol{u}\_1, \cdots, \boldsymbol{u}\_m)$ where $\boldsymbol{u}\_i \in V$. For any $\boldsymbol{w} \in W$, $\boldsymbol{w} =\sum\_{i=1}^m c_i \boldsymbol{u}_i \in V$ because $V$ is a vector space. Also for any $\boldsymbol{w}, \boldsymbol{v} \in W$,

$$
a\boldsymbol{w} + b\boldsymbol{v} = a\sum_{i=1}^m c_i \boldsymbol{u}_i + b\sum_{i=1}^m d_i \boldsymbol{u}_i = \sum_{i=1}^m (ac_i + bd_i)\boldsymbol{u}_i,
$$

which is a linear combination of the $\boldsymbol{u}_i$'s in $W$.

### Spanning set

Let $V$ be a vector space (ambient space or a subspace of that). Suppose there exists a set of vectors $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$ in $V$ such that for any $\boldsymbol{u} \in V$, we can express $\boldsymbol{u}$ as a linear combination of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. Then we say $\\{ \boldsymbol{u}_1, \cdots, \boldsymbol{u}_m \\}$ is a `spanning set` of $V$.

Suppose $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is a spanning set of $V$, then $\mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) = V$. The LHS is the set of all possible linear combinations of the $\boldsymbol{u}_i$'s. As $V$ is closed under linear combination, LHS $\subset$ RHS. For all $\boldsymbol{v} \in V$, as $S$ is a spanning set, $\boldsymbol{v}$ can be expressed as a linear combination of the $\boldsymbol{u}_i$'s. Since the LHS contains all possible linear combinations of the $\boldsymbol{u}_i$'s, $\boldsymbol{v} \in LHS$, thus RHS $\subset$ LHS.

A vector space is associated with a _finite_ number of vectors. For example, $\mathbb{R}^3$ has a spanning set of {$(1, 0, 0)^\prime$, $(0, 1, 0)^\prime$, $(0, 0, 1)^\prime$}. It's important to know that {{<hl>}}a spanning set is not unique.{{</hl>}} {$(1, 1, 1)^\prime$, $(1, 1, 0)^\prime$, $(0, 1, 1)^\prime$, $(1, 0, 0)^\prime$} is also a valid spanning set for $\mathbb{R}^3$. In fact, there's a infinite number of spanning sets for a vector space.

## Basis

A `basis` is nothing but a linearly independent spanning set. $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is a basis of $V$ if $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is linearly independent, and $\mathcal{L}(\boldsymbol{u}\_1, \cdots, \boldsymbol{u}\_m) = V$, i.e. it's a spanning set of $V$.

In general, a vector space $V$ has infinite bases. But all bases have the same number of vectors, called the `dimension` of $V$. The dimension of $V$ is defined as the maximum number of linearly independent vectors in $V$.

For example, $\mathbb{R}^3$ has a basis $\\{(1, 1, 1)^\prime, (1, 1, 0)^\prime, (0, 1, 1)^\prime\\}$. We can easily show that the set of vectors are LIN. We can also show that $\forall (x, y, z)^\prime \in \mathbb{R}^3$, it can be expressed as a linear combination of the three vectors:

$$
\begin{aligned}
    (x, y, z)^\prime &= a_1 \boldsymbol{u}_1 + a_2 \boldsymbol{u}_2 + a_3 \boldsymbol{u}_3 \\\\
    &= (a_1 + a_2, a_1 + a_2 + a_3, a_1 + a_3)^\prime \\\\
    &\Rightarrow a_3 = y - x, a_1 = z - y + x, a_2 = y - z
\end{aligned}
$$

Given a vector space $W$, e.g. $W = \\{(x, y, z)^\prime \mid y - 2x = 0\\}$, to find a basis of $W$, we first need to find two LIN vectors from $W$. Then, show that

$$
(x, 2x, z)^\prime = x(1, 2, 0)^\prime + z(0, 0, 1)^\prime
$$

to prove $\\{(1, 2, 0)^\prime, (0, 0, 1)^\prime\\}$ is a basis of $W$.

**Theorem:** Every vector in $V$ has a unique representation in terms of a given basis.

Suppose $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is a basis of $V$. Suppose there exists two sets of coefficients $\\{a_1, \cdots, a_n\\}$ and $\\{b_1, \cdots, b_n\\}$ such that $\boldsymbol{x} = \sum a_i\boldsymbol{u}_i$ and $\boldsymbol{x} = \sum b_i \boldsymbol{u}_i$ where $a_i \neq b_i$ for some $i$. If we subtract one from the other,

$$
\boldsymbol{0} = \sum_{i=1}^n (a_i - b_i) \boldsymbol{u}_i = \sum_{i=1}^n c_i \boldsymbol{u}_i
$$

As the $\boldsymbol{u}_i$'s are linearly independent, $c_i = 0 \forall i$, i.e. $a_i = b_i \forall i$.

## Dimension

When we say the dimension of a vector space, it's not the same as the number of entries in a vector. For $V = \\{\boldsymbol{0}\\}$, the dimension of $V$ is $dim(V) = 0$. Otherwise, the `dimension` of $V$ is the number of vectors in any basis of $V$.

For $V = \mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m)$, $dim(V) = m$.

### Examples

The dimension of $\mathbb{R}^n = n$. A possible basis is

$$
\\{(1, 0, \cdots, 0)^\prime, (0, 1, 0, \cdots, 0)^\prime, \cdots, (0, 0, \cdots, 0, 1)^\prime\\}
$$

For $V = \mathcal{L}(\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3)$ where

$$
\boldsymbol{u}_1 = (1, 1, 1)^\prime, \boldsymbol{u}_2 = (1, 0, -1)^\prime, \boldsymbol{u}_3 = (3, 2, 1)^\prime
$$

We construct linear combinations such that $\sum \alpha_i \boldsymbol{u}_i = 0$:

$$
\begin{cases}
    \alpha_1 + \alpha_2 + 3\alpha_3 = 0 \\\\
    \alpha1 + 2 \alpha_3 = 0 \\\\
    \alpha_1 - \alpha_2 + \alpha_3 = 0
\end{cases} \Rightarrow (\alpha_1, \alpha_2, \alpha_3) = (2\alpha_2, \alpha_2, -\alpha_2)
$$

There exists infinite solutions such as $(2, 1, -1)$, $(4, 2, -2)$ etc., so $\\{\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3\\}$ is linearly dependent, and $dim(V) < 3$.

We can see that $\\{\boldsymbol{u}_1, \boldsymbol{u}_2\\}$ is linearly independent, so they form a basis and $dim(V) = 2$.

The third example is $W = \\{(x, y, z)^\prime \mid 2x - y = 0 \\}$. We have a linear restriction on $x$ and $y$, so the dimension of $W$ is $3-1=2$. Two possible methods are

1. find two LIN vectors in $W$ that form a basis, such as $\\{(0, 0, 1)^\prime, (1, 2, 0)^\prime\\}$. We need to show that these are LIN, and they span $W$.
2. $(x, y, z)^\prime = (x, 2x, z)^\prime = x(1, 2, 0)^\prime + z(0, 0, 1)^\prime$.

The second method is generalizable. Let $W = \\{(x, y, z, w)^\prime \mid x - y + w = 0 \\}$.

$$
(y-w, y, z, w)^\prime = y(1, 1, 0, 0)^\prime + z(0, 0, 1, 0)^\prime + w(-1, 0, 0, 1)^\prime
$$

If we add another constraint of $z = 2w$, then

$$
(y-w, y, 2w, w)^\prime = y(1, 1, 0, 0)^\prime + w(-1, 0, 2, 1)^\prime
$$

The dimension is the dimension of the ambient space $dim(\mathbb{R}^4) = 4$ minus the number of LIN restrictions.

### Facts

1. Every non-null vector space $V$ of finite dimension has a basis.
2. Any two bases of $V$ must have the same number of vectors.
3. If $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is linearly independent, $\boldsymbol{u}_i \in V$, and the dimension of $V$ is $n$, then $S$ is a basis.
4. If $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ spans $V$ and $dim(V) = n$, then $S$ is linearly independent and a basis of $V$.
5. If $W$ is a subspace of $V$ and $dim(W) = dim(V)$, then $W = V$.
6. What if $W_1$ and $W_2$ are both subspaces of $V$, and $dim(W_1) = dim(W_2)$? Is $W_1 = W_2$? The answer is **no**.
7. If $dim(V) = n$, and $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_r\\} \subset V$ are linearly independent, then we can find a basis that contains this set as a subset.

## Inner product

The `inner product` or `dot product` is a function that multiplies vectors together into a scalar. The inner product between two vectors is defined as

$$
\langle \boldsymbol{u},\boldsymbol{v} \rangle = \boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{u}^\prime \boldsymbol{v} = \boldsymbol{v}^\prime \boldsymbol{u} = \sum_{i=1}^p u_i v_i
$$

where $\boldsymbol{u} \in V$, $\boldsymbol{v} \in V$ and $V$ is a subspace in $\mathbb{R}^p$. Some properties of the inner product are:

1. $\boldsymbol{u} \cdot \boldsymbol{u} \geq 0$, and $\boldsymbol{u} \cdot \boldsymbol{u} = 0 \Leftrightarrow \boldsymbol{u} = \boldsymbol{0}$.
2. $\boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{v} \cdot \boldsymbol{u}$.
3. $(\boldsymbol{u} + \boldsymbol{v}) \cdot \boldsymbol{w} = \boldsymbol{u} \cdot \boldsymbol{w} + \boldsymbol{v} \cdot \boldsymbol{w}$.
4. $(c\boldsymbol{u}) \cdot \boldsymbol{v} = c(\boldsymbol{u} \cdot \boldsymbol{v})$.

## Norm

The norm of a vector measures the "size" of a vector. Any function that satisfies the following conditions is considered a `norm`:

1. $\\|\boldsymbol{u}\\| \geq 0$.
2. $\\|\boldsymbol{u}\\| = 0$ if $\boldsymbol{u} = \boldsymbol{0}$.
3. $\\|\alpha \boldsymbol{u}\\| = |\alpha| \\|\boldsymbol{u}\\|$.
4. $\\|\boldsymbol{u} + \boldsymbol{v}\\| \leq \\|\boldsymbol{u}\\| + \\|\boldsymbol{v}\\|$.

In this class, we specifically use the `Euclidean norm` ($L_2$ norm), which is defined as

$$
\\|\boldsymbol{u}\\|\_2 = \sqrt{\boldsymbol{u} \cdot \boldsymbol{u}} = \sqrt{\sum\_{i=1}^n u_i^2}
$$

Another commonly used norm is the $L_1$ norm, which is

$$
\\|\boldsymbol{u}\\|_1 = \sum_{i=1}^n |u_i|
$$

In general, the $L_p$ norm is defined as

$$
\\|\boldsymbol{u}\\|_p = \left(\sum_{i=1}^n |u_i|^p \right)^\frac{1}{p}
$$

and the norms for $0 \leq p \leq 2$ are extensively studied. The `sup-norm` is

$$
\\|\boldsymbol{u}\\|_\infty = \max_{1 \leq i \leq n} |u_i|
$$

and finally the $L_0$ norm is

$$
\\|\boldsymbol{u}\\|_0 = \text{ \# of non-zero elements}
$$

Using the vector $\boldsymbol{u} - (1, 0, 3, -4)^\prime$ as an example, the norms are

$$
\begin{aligned}
    &\\|\boldsymbol{u}\\|_2 = \sqrt{1 + 9 + 16}, &&\\|\boldsymbol{u}\\|_1 = 1 + 3 + 4, \\\\
    &\\|\boldsymbol{u}\\|_\infty = 4, &&\\|\boldsymbol{u}\\|_0 = 3
\end{aligned}
$$

## Distance

The distance between two vectors is denoted $d(\boldsymbol{u}, \boldsymbol{v})$, and has to satisfy the following conditions:

1. $d(\boldsymbol{u}, \boldsymbol{v}) \geq 0$.
2. $d(\boldsymbol{u}, \boldsymbol{v}) = 0$ if and only if $\boldsymbol{u} = \boldsymbol{v}$.
3. $d(\boldsymbol{u}, \boldsymbol{v}) = d(\boldsymbol{v}, \boldsymbol{u})$.
4. Triangular inequality: $d(\boldsymbol{u}, \boldsymbol{w}) \leq d(\boldsymbol{u}, \boldsymbol{v}) + d(\boldsymbol{v}, \boldsymbol{w})$.

The `Euclidean distance` ($L_2$ distance) is the most common choice:

$$
\\|\boldsymbol{u} - \boldsymbol{v}\\|_2 = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
$$

It has a very natural interpretation: imagine a line connecting the two endpoints of the vectors. The $L_2$ distance is the length of this line[^l1-distance].

{{< figure src="distances.png" caption="The red line demonstrates the $L_2$ distance." numbered="true" >}}

[^l1-distance]: For the `Manhattan distance` or $L_1$ distance, think of it as travelling from $\boldsymbol{u}$ to $\boldsymbol{v}$ but you can only move horizontally or vertically.

### Triangular inequality

Let's see if the $L_2$ norm satisfies the triangular inequality:

$$
\\|\boldsymbol{u} + \boldsymbol{v}\\|_2 \leq \\|\boldsymbol{u}\\|_2 + \\|\boldsymbol{v}\\|_2
$$

As both sides are $\geq 0$, we will prove

$$
\\|\boldsymbol{u} + \boldsymbol{v}\\|_2^2 \leq \left(\\|\boldsymbol{u}\\|_2 + \\|\boldsymbol{v}\\|_2 \right)^2
$$

$$
\begin{gathered}
    LHS = \sum_{i=1}^n (u_i + v_i)^2 = \sum u_i^2 + \sum v_i^2 + 2 \sum u_i v_i
    = \\|\boldsymbol{u}\\|^2 + \\|\boldsymbol{v}\\|^2 + 2\boldsymbol{u}\cdot\boldsymbol{v} \\\\
    RHS = \\|\boldsymbol{u}\\|^2 + \\|\boldsymbol{v}\\|^2 + 2 \\|\boldsymbol{u}\\| \\|\boldsymbol{v}\\|
\end{gathered}
$$

Thus we need to prove $\\|\boldsymbol{u}\\| \\|\boldsymbol{v}\\| \geq \boldsymbol{u} \cdot \boldsymbol{v}$. If $\boldsymbol{u} \cdot \boldsymbol{v} < 0$, the inequality is obvious. When $\boldsymbol{u} \cdot \boldsymbol{v} \geq 0$, we may use the `Cauchy-Schwarz inequality`:

$$
(\boldsymbol{u} \cdot \boldsymbol{v})^2 \leq \\|\boldsymbol{u}\\|^2 \\|\boldsymbol{v}\\|^2
$$

The "high-school" version of this is

$$
(ax + by)^2 \leq (a^2 + b^2)(x^2 + y^2)
$$

This is a special case with $\boldsymbol{u} = (a, b)^\prime$ and $\boldsymbol{v} = (x, y)^\prime$. The equality happens when $bx = ay$, i.e. $\boldsymbol{u} \propto \boldsymbol{v}$.

In terms of real numbers, Cauchy-Schwarz tells us that

$$
\left(\sum u_iv_i \right)^2 \leq \left(\sum u_i^2 \right) \left(\sum v_i^2 \right)
$$

The geometric interpretation of this is as follows.

{{< figure src="law_of_cosine.png" caption="Geometric interpretation of the triangular inequality." numbered="true" >}}

The `law of cosine` tells us that

$$
\begin{aligned}
    \\|\boldsymbol{x} - \boldsymbol{y}\\|^2 &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 - 2\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha) \\\\
    \\|\boldsymbol{x} - \boldsymbol{y}\\|^2 &= (\boldsymbol{x} - \boldsymbol{y})^\prime(\boldsymbol{x} - \boldsymbol{y}) \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 - 2\boldsymbol{x}^\prime \boldsymbol{y} \\\\
    \Rightarrow \boldsymbol{x}^\prime\boldsymbol{y} &= \\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha)
\end{aligned}
$$

We get

$$
\cos(\alpha) = \frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|}
$$

In our example figure above, we can find $\cos(\alpha) = \frac{24-2}{\sqrt{20}\sqrt{37}}$.

As $-1 \leq \cos(\alpha) \leq 1$,

$$
\begin{gathered}
    -1 \leq \frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|} \leq 1 \\\\
    0 \leq \left(\frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|} \right)^2 \leq 1 \\\\
    (\boldsymbol{x}^\prime \boldsymbol{y})^2 \leq \\|\boldsymbol{x}\\|^2 \\|\boldsymbol{y}\\|^2
\end{gathered}
$$

which is the Cauchy-Schwarz inequality. The equality happens when $\alpha = 0$. If $\alpha = 90^\circ$, $\cos(\alpha) = 0$ and $\boldsymbol{x}^\prime \boldsymbol{y} = 0$.

What about $\boldsymbol{x} + \boldsymbol{y}$?

$$
\begin{aligned}
    \\|\boldsymbol{x} + \boldsymbol{y}\\|^2 &= (\boldsymbol{x} + \boldsymbol{y})^\prime (\boldsymbol{x} + \boldsymbol{y}) \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 + 2 \boldsymbol{x}^\prime \boldsymbol{y} \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 + 2\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha)
\end{aligned}
$$

If $\alpha = 90^\circ$, $\\|\boldsymbol{x} + \boldsymbol{y}\\|^2 = \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2$.

## Orthogonality

For vectors $\boldsymbol{u}, \boldsymbol{v} \in \mathbb{R}^n$, we say they are `orthogonal` if

$$
\boldsymbol{u} \cdot \boldsymbol{v} = \langle \boldsymbol{u}, \boldsymbol{v} \rangle = \boldsymbol{u}^T\boldsymbol{v} = \sum_{i=1}^n u_i v_i = 0,
$$

which is denoted $\boldsymbol{u} \perp \boldsymbol{v}$.

### Orthogonal basis

Let a set of non-zero vectors $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ be a basis for $V$. If $\boldsymbol{u}_i \perp \boldsymbol{u}_j$ for all $i \neq j$, then we say $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is an `orthogonal basis` of $V$.

**Corollary:** The $\boldsymbol{u}_i$'s are linearly independent. To prove this, we set $\sum \alpha_i \boldsymbol{u}_i = \boldsymbol{0}$. Consider the inner product between $\sum \alpha_i \boldsymbol{u}_i$ and $\boldsymbol{u}_1$:

$$
\begin{aligned}
    0 &= \langle \boldsymbol{0}, \boldsymbol{u}_1 \rangle = \langle \sum \alpha_i \boldsymbol{u}_i, \boldsymbol{u}_1 \rangle \\\\
    &= \sum \alpha_i \langle \boldsymbol{u}_i, \boldsymbol{u}_1 \rangle \quad \text{(distributive, associative)} \\\\
    &= \alpha_1 \langle \boldsymbol{u}_1, \boldsymbol{u}_1 \rangle + \alpha_2 \langle \boldsymbol{u}_2, \boldsymbol{u}_1 \rangle + \cdots + \alpha_n \langle \boldsymbol{u}_n, \boldsymbol{u}_1 \rangle \\\\
    &= \alpha_1 \\|\boldsymbol{u}_1 \\|^2,
\end{aligned}
$$

which implies either $\alpha_1 = 0$ or $\\|\boldsymbol{u}_1 \\|^2 = 0$. Here $\alpha_1$ must be zero because we've stated all the $\boldsymbol{u}_i$'s are non-zero vectors. Repeat this and replace $\boldsymbol{u}_1$ with $\boldsymbol{u}_2, \cdots, \boldsymbol{u}_n$, we can show that all the $\alpha$'s are zero, which proves linear independence.

Orthogonal bases are **not unique**. For example, $\mathbb{R}^3$ has

$$
\begin{gathered}
    \\{(1, 0, 0)^\prime, (0, 1, 0)^\prime, (0, 0, 1)^\prime\\} \\\\
    \\{(1, 1, 0)^\prime, (1, -1, 0)^\prime, (0, 0, 1)^\prime\\} \\\\
    \vdots
\end{gathered}
$$

### Normalization

For any non-zero vector $\boldsymbol{u} \in \mathbb{R}^n$, the `normalized vector` of $\boldsymbol{u}$ is

$$
\boldsymbol{e} = \frac{\boldsymbol{u}}{\\|\boldsymbol{u}\\|} = \frac{1}{\\|\boldsymbol{u}\\|}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n)
$$

This is also called the `unit vector`, and the unit norm is 1:

$$\\|\boldsymbol{e}\\| = \left\\| \frac{\boldsymbol{u}}{\\|\boldsymbol{u}\\|} \right\\| = \frac{1}{\\|\boldsymbol{u}\\|} \cdot \\|\boldsymbol{u}\\| = 1$$

For example, let $\boldsymbol{u} = (2, 3, -1)^\prime$. the norm of $\boldsymbol{u}$ is

$$
\\|\boldsymbol{u}\\| = \sqrt{4 + 9 + 1} = \sqrt{14}
$$

and the normalized vector is

$$
\boldsymbol{e} = \frac{\boldsymbol{u}}{\\|\boldsymbol{u}\\|} = \left( \frac{2}{\sqrt{14}}, \frac{3}{\sqrt{14}}, -\frac{1}{\sqrt{14}} \right)^\prime
$$

`Orthonormal basis` consists of normalized, orthogonal vectors.

## Projection

Now that all the basic definitions are given, we're ready to talk about the most important part in this chapter: projection.

### Projection (of a vector) onto a vector

For vectors $\boldsymbol{u}, \boldsymbol{v} \in V$, $P(\boldsymbol{u} \mid \boldsymbol{v})$ is called the `projection` of $\boldsymbol{u}$ onto $\boldsymbol{v}$ if:

1. $P(\boldsymbol{u} \mid \boldsymbol{v})$ is proportional to $\boldsymbol{v}$, i.e. $P(\boldsymbol{u} \mid \boldsymbol{v}) = b\boldsymbol{v}$ for some $b \in \mathbb{R}$.
2. $\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v})$ is orthogonal to $\boldsymbol{v}$.

> TODO: figure of projecting a vector onto another vector.

To find the projection, we just need to find the scalar $b$. By condition 2, $u\boldsymbol{u} - b\boldsymbol{v} \perp \boldsymbol{v}$, so if we take the inner product:

$$
\begin{gathered}
    (\boldsymbol{u} - b\boldsymbol{v})^\prime \boldsymbol{v} = \boldsymbol{u}^\prime \boldsymbol{v} - b \boldsymbol{v}^\prime \boldsymbol{v} = 0 \\\\
    \boldsymbol{u}^\prime \boldsymbol{v} - b \\|\boldsymbol{v}\\|^2 = 0 \\\\
    b = \frac{\boldsymbol{u}^\prime \boldsymbol{v}}{\\|\boldsymbol{v}\\|^2}
\end{gathered}
$$

In general,

$$
P(\boldsymbol{u} \mid \boldsymbol{v}) = \left( \frac{\boldsymbol{u}^\prime \boldsymbol{v}}{\\|\boldsymbol{v}\\|^2} \right) \boldsymbol{v}
$$

For example, if $\boldsymbol{u} = (1, 1, 2, -1)^\prime$ and $\boldsymbol{v} = (0, 2, -2, 1)^\prime$, the projection of $\boldsymbol{u}$ onto $\boldsymbol{v}$ is

$$
\begin{aligned}
    P(\boldsymbol{u} \mid \boldsymbol{v}) &= \frac{2 - 4 - 1}{4 + 4 + 1} (0, 2, -2, 1)^\prime \\\\
    &= -\frac{1}{3} (0, 2, -2, 1)^\prime = \left( 0, -\frac{2}{3}, \frac{2}{3}, -\frac{1}{3} \right)^\prime
\end{aligned}
$$

We can also show $\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v}) \perp \boldsymbol{v}$ by showing that:

$$
(\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v}))^\prime \boldsymbol{v} = 0 \Leftrightarrow \boldsymbol{u}^\prime \boldsymbol{v} = P(\boldsymbol{u} \mid \boldsymbol{v})^\prime \boldsymbol{v}
$$

In other words, the inner product between $\boldsymbol{u}$ and $\boldsymbol{v}$ is equal to the inner product between the projection of $\boldsymbol{u}$ onto $\boldsymbol{v}$ and $\boldsymbol{v}$.

$$
\begin{aligned}
    P(\boldsymbol{u} \mid \boldsymbol{v}) &= \\|P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 + \\|\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 \\\\
    &= \\|b\boldsymbol{v}\\|^2 + \\|\boldsymbol{u} - b\boldsymbol{v}\\|^2 \\\\
    &= b^2\\|\boldsymbol{v}\\|^2 + \\|\boldsymbol{u}\\|^2 + b^2 \\|\boldsymbol{v}\\|^2 - 2b\boldsymbol{u}^\prime\boldsymbol{v} \\\\
    &= 2\left( \frac{\boldsymbol{u}^\prime \boldsymbol{v}}{\\|\boldsymbol{v}\\|^2} \right)^2 \\|\boldsymbol{v}\\|^2 + \\|\boldsymbol{u}\\|^2 - 2 \frac{(\boldsymbol{u}^\prime \boldsymbol{v})^2}{\\|\boldsymbol{v}\\|^2} \\\\
    &= \\|\boldsymbol{u}\\|^2
\end{aligned}
$$

#### Cauchy-Schwarz inequality

By the Pythagorean theorem,

$$
\begin{aligned}
    \\|\boldsymbol{u}\\|^2 &= \\|P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 + \\|\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 \\\\
    & \geq \\|P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 \\\\
    &= \\|b\boldsymbol{v}\\|^2 = \frac{(\boldsymbol{u}^\prime \boldsymbol{v})^2}{\\|\boldsymbol{v}\\|^2}
\end{aligned}
$$

Thus,

$$
\\|\boldsymbol{u}\\|^2 \geq \frac{(\boldsymbol{u}^\prime \boldsymbol{v})^2}{\\|\boldsymbol{v}\\|^2} \Leftrightarrow (\boldsymbol{u}^\prime \boldsymbol{v})^2 \leq \\|\boldsymbol{u}\\|^2 \\|\boldsymbol{v}\\|^2
$$

#### Meaning of projection

**Theorem:** Among all scalar multiples of $\boldsymbol{v}$, $a\boldsymbol{v}$ $a \in \mathbb{R}$, the projection of $\boldsymbol{u}$ onto $\boldsymbol{v}$ is the closest[^closest-vector] vector to $\boldsymbol{u}$.

[^closest-vector]: Closest meaning that having the shortest distance to.

To prove this, we will show

$$
\underset{a}{\text{argmin}} \\|\boldsymbol{u} - a\boldsymbol{v}\\|^2 = \frac{\boldsymbol{u}^\prime\boldsymbol{v}}{\\|\boldsymbol{v}\\|^2}
$$

Here we're minimizing the squared distance between $\boldsymbol{u}$ and $a\boldsymbol{v}$.

$$
\begin{aligned}
    \\|\boldsymbol{u} - a\boldsymbol{v}\\|^2 &= \\|\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v}) + P(\boldsymbol{u} \mid \boldsymbol{v}) - a\boldsymbol{v}\\|^2 \\\\
    &= \\|\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 + \\|P(\boldsymbol{u} \mid \boldsymbol{v}) - a\boldsymbol{v}\\|^2 + \underbrace{2(u - P(\boldsymbol{u} \mid \boldsymbol{v}))^\prime(P(\boldsymbol{u} \mid \boldsymbol{v}) - a\boldsymbol{v})}_{=0}
\end{aligned}
$$

Note that the first term doesn't contain $a$, so we don't have to consider it when minimizing w.r.t. $a$. The second term $P(\boldsymbol{u} \mid \boldsymbol{v}) - a\boldsymbol{v}$ becomes zero when $a\boldsymbol{v} = P(\boldsymbol{u} \mid \boldsymbol{v})$, attaining its minimum value. Thus,

$$
\min \\|\boldsymbol{u} - a\boldsymbol{v}\\|^2 = \min \\|\boldsymbol{u} - P(\boldsymbol{u} \mid \boldsymbol{v})\\|^2 \qquad \text{when } a = \frac{\boldsymbol{u}^\prime\boldsymbol{v}}{\\|\boldsymbol{v}\\|^2}
$$

For example, for vector $\boldsymbol{y} = (y_1, \cdots, y_n)^\prime \in \mathbb{R}^n$, our goal is to find a scalar $a$ that is closet to $\boldsymbol{y}$, i.e. find $a \boldsymbol{1}_n = (a, a, \cdots, a)^\prime$ that is closest to $\boldsymbol{y}$.

$$
\begin{aligned}
    P(\boldsymbol{y} \mid \boldsymbol{1}_n) &= \left( \frac{\boldsymbol{y}^\prime \boldsymbol{1}_n}{1^2 + 1^2 + \cdots + 1^2} \right)\boldsymbol{1}_n \\\\
    &= \frac{y_1 + y_2 + \cdots + y_n}{n} \boldsymbol{1}_n \\\\
    &= \bar{y} \boldsymbol{1}_n
\end{aligned}
$$

Therefore $\bar{y}$ is closest to the data $(y_1, \cdots, y_n)^\prime$, or

$$
\bar{y} = \underset{a}{\text{argmin}} \sum_{i=1}^n (y_i - a)^2 = \underset{a}{\text{argmin}} \\|\boldsymbol{y} - a\boldsymbol{1}_n\\|^2
$$

Geometrically speaking, the sample mean deviance vector is orthogonal to the vector of sample means:

$$
\begin{pmatrix}
    y_1 - \bar{y} \\\\ y_2 - \bar{y} \\\\ \vdots \\\\ y_n - \bar{y}
\end{pmatrix} \perp
\begin{pmatrix}
    \bar{y} \\\\ \bar{y} \\\\ \vdots \\\\ \bar{y}
\end{pmatrix} \Leftrightarrow \frac{1}{n-1}\sum_{i=1}^n (y_i - \bar{y}) \perp \bar{y}
$$

This means the sample variance is uncorrelated with the sample mean. In terms of repeated sampling, if we plot the variance of each sample against the sample means, there should be no patterns.

### Subspace projection

Vector $\boldsymbol{x}$ is `orthogonal to a vector space` $V$ if $\boldsymbol{x} \perp \boldsymbol{y}$ for all $\boldsymbol{y} \in V$. $P(\boldsymbol{u} \mid V)$ is called the `projection of a vector onto a subspace` if

1. $P(\boldsymbol{u} \mid V) \in V$, and
2. $\boldsymbol{u} - P(\boldsymbol{u} \mid V) \perp V$.

In terms of inner products, we need to have

$$
\boldsymbol{u} \cdot \boldsymbol{v} = P(\boldsymbol{u} \mid V) \cdot \boldsymbol{v}
$$

**Lemma:** Suppose $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ spans $V$, then

$$
\boldsymbol{x} \perp V \Leftrightarrow \boldsymbol{x} \perp \boldsymbol{u}_j, \quad j = 1, \cdots, n
$$

In other words, we only need to see the orthogonality with vectors in a spanning set.

Proving $\Rightarrow$ is trivial because it's true by definition of orthogonality to a vector space. To prove $\Leftarrow$, for all $\boldsymbol{v} \in V$, we will show if $\boldsymbol{x} \perp \boldsymbol{u}_j$, then $\boldsymbol{x} \perp \boldsymbol{v}$.

As $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is a spanning set, we can express $\boldsymbol{v}$ as a linear combination of the $\boldsymbol{u}$'s:

$$
\boldsymbol{v} = \sum_{j=1}^n \alpha_j \boldsymbol{u}_j \quad \text{for some } \alpha_j
$$

The inner product between $\boldsymbol{x}$ and $\boldsymbol{v}$ is

$$
\begin{aligned}
    \boldsymbol{x} \cdot \boldsymbol{v} &= \boldsymbol{x} \cdot \left( \sum_{j=1}^n \alpha_j \boldsymbol{u}_j \right) \\\\
    &= \sum_{j=1}^n \alpha_j (\boldsymbol{x} \cdot \boldsymbol{u}_j) \\\\
    &= \sum_{j=1}^n \alpha_j \cdot 0 = 0
\end{aligned}
$$

#### Theorem

Suppose $V$ is a subspace of $\mathbb{R}^n$, and $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_k\\}$ is an **orthogonal basis** for $V$. For $\boldsymbol{u} \in \mathbb{R}^n$, the `projection` of $\boldsymbol{u}$ onto subspace $V$ is

$$
P(\boldsymbol{u} \mid V) = \sum_{i=1}^k P(\boldsymbol{u} \mid \boldsymbol{u}_i) = \sum_{i=1}^k \frac{\boldsymbol{u}^\prime \boldsymbol{u}_i}{\\|\boldsymbol{u}_i\\|^2} \boldsymbol{u}_i
$$

{{< figure src="projection_on_subspace.png" caption="Illustration of a vector $\boldsymbol{u}$ projected onto subspace $V$." numbered="true" >}}

In the case of an orthonormal basis, this can be further simplified to $\sum_{i=1}^k (\boldsymbol{u}^\prime \boldsymbol{u}_i)\boldsymbol{u}_i$.

To prove this, let $\hat{\boldsymbol{u}} = P(\boldsymbol{u} \mid V)$[^why-u-hat] and $\hat{\boldsymbol{u}}\_i = P(\boldsymbol{u} \mid \boldsymbol{u}\_i)$. The theorem says $\hat{\boldsymbol{u}} = \sum_{i=1}^k \hat{\boldsymbol{u}}\_i$. We need to show $\sum_{i=1}^k \hat{\boldsymbol{u}}\_i$ satisfies

1. $\hat{\boldsymbol{u}} \in V$.
2. For all $\boldsymbol{v} \in V$, $(\boldsymbol{u} - \hat{\boldsymbol{u}}) \cdot \boldsymbol{v} = 0$, or $(\boldsymbol{u} - \sum \hat{\boldsymbol{u}}\_i ) \cdot \boldsymbol{v} = 0$.

[^why-u-hat]: We use the hat notation here because a prediction (e.g. in linear regression) is nothing but a projection.

First we write $\boldsymbol{v} = \sum \alpha_i \boldsymbol{u}_i$. Then,

$$
\begin{aligned}
    \left(\boldsymbol{u} - \sum_{i=1}^k \hat{\boldsymbol{u}}\_i \right) \cdot \boldsymbol{v} &= \boldsymbol{u} \cdot \boldsymbol{v} - \sum_{i=1}^k \hat{\boldsymbol{u}}_i \cdot \boldsymbol{v} \\\\
    &= \boldsymbol{u} \cdot \left( \sum_{i=1}^k \alpha_i \boldsymbol{u}_i \right) - \sum_{i=1}^k \left( \hat{\boldsymbol{u}}_i \cdot \left( \sum_{j=1}^k \alpha_j \boldsymbol{u}_j \right) \right) \\\\
    &= \sum_{i=1}^k \alpha_i (\boldsymbol{u} \cdot \boldsymbol{u}_i) - \sum_{i=1}^k \sum_j \alpha_{j=1}^k (\underbrace{\hat{\boldsymbol{u}}_i \cdot \boldsymbol{u}_j}_{\text{all zeros except } i=j}) \\\\
    &= \sum_{i=1}^k \alpha_i (\boldsymbol{u} \cdot \boldsymbol{u}_i) - \sum_{i=1}^k \alpha_i (\hat{\boldsymbol{u}}_i \cdot \boldsymbol{u}_i) \\\\
    &= 0 \text{ because } \boldsymbol{u} \cdot \boldsymbol{u}_i = \hat{\boldsymbol{u}}_i \cdot \boldsymbol{u}_i
\end{aligned}
$$

#### Example

Let $V = \mathcal{L}(\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3)$ where

$$
\begin{gathered}
    \boldsymbol{u}_1 = (1, 1, 1, 0, 0, 0)^\prime \\\\
    \boldsymbol{u}_2 = (0, 0, 0, 1, 1, 0)^\prime \\\\
    \boldsymbol{u}_3 = (0, 0, 0, 0, 0, 1)^\prime
\end{gathered}
$$

We have $\boldsymbol{u} = (6, 10, 5, 4, 8, 7)^\prime \in \mathbb{R}^6$ and want to find the projection of $\boldsymbol{u}$ onto $V$.

$$
\begin{aligned}
    P(\boldsymbol{u} \mid V) &= P(\boldsymbol{u} \mid \boldsymbol{u}_1) + P(\boldsymbol{u} \mid \boldsymbol{u}_2) + P(\boldsymbol{u} \mid \boldsymbol{u}_3) \\\\
    &= \frac{6 + 10 + 5}{3}\boldsymbol{u}_1 + \frac{4 + 8}{2}\boldsymbol{u}_2 + \frac{7}{1}\boldsymbol{u}_3 \\\\
    &= 7\boldsymbol{u}_1 + 6\boldsymbol{u}_2 + 7\boldsymbol{u}_3 \\\\
    &= (7, 7, 7, 6, 6, 7)^\prime
\end{aligned}
$$

This is the vector in $V$ that's the closest to $\boldsymbol{u}$.

If the given basis was orthonormal:

$$
\begin{gathered}
    \boldsymbol{z}_1 = (\frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}}, 0, 0, 0)^\prime \\\\
    \boldsymbol{z}_2 = (0, 0, 0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0)^\prime \\\\
    \boldsymbol{z}_3 = (0, 0, 0, 0, 0, 1)^\prime
\end{gathered}
$$

The projection

$$
\begin{aligned}
    P(\boldsymbol{u} \mid V) &= (\boldsymbol{u} \cdot \boldsymbol{z}_1) \boldsymbol{z}_1 + (\boldsymbol{u} \cdot \boldsymbol{z}_2) \boldsymbol{z}_2 + (\boldsymbol{u} \cdot \boldsymbol{z}_3) \boldsymbol{z}_3 \\\\
    &= \frac{6+10+5}{\sqrt{3}}\boldsymbol{z}_1 + \frac{4+8}{\sqrt{2}}\boldsymbol{z}_2 + 7\boldsymbol{z}_3 \\\\
    &= (7, 7, 7, 6, 6, 7)^\prime
\end{aligned}
$$

#### Orthogonalization

So how do we make a basis to an orthogonal basis? This is the whole idea of the `Gram-Schmidt orthogonalization`. We start with a non-orthogonal basis $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_k\\}$ for $V$, and our goal is to find $\\{\boldsymbol{v}_1, \cdots, \boldsymbol{v}_k\\}$, an orthogonal basis for $V$.

1. The first member in the orthogonal basis is the same as the first member in the given basis: $\boldsymbol{v}_1 = \boldsymbol{u}_1$.
