---
title: "Vector Space"
date: 2020-08-31T13:13:34-04:00
summary: "" # appears in list of posts
categories: ["Linear Algebra"] # main category; shown in post metadata
tags: [] # list of related tags

slug: "linear-algebra-vector-space"
toc: true # table of contents button in post

# featured posts are shown on the homepage
featured: false
draft: true

weight: 30 # smaller values are listed first

# full-width featured image
# To use, add an image named `featured.jpg/png` to your page's folder, or
# fill the unsplash_id and the photo will be automatically retrieved.
header_image:
    caption: "Ocean clouds seen from space." # Give credits here, or whatever captions you want to add (support markdown)
    unsplash_id: "yZygONrUBe8" # Unsplash ID of the picture
---

Associated with any matrix is a very important characteristic called the rank. There are several ways
of defining the rank, and the most fundamental of these is in terms of the dimension of a linear space.

## Vector space

A non-empty set of vectors $V$ is called a `vector space` (or linear space) if:

1. For all $\boldsymbol{x}, \boldsymbol{y} \in V$, $\boldsymbol{x} + \boldsymbol{y} \in V$, i.e. $V$ is closed under addition.
2. For all $\boldsymbol{x} \in V$, $k \in \mathbb{R}$, $k\boldsymbol{x} \in V$, i.e. $V$ is closed under scalar multiplication.

The second condition implies that the zero vector is always included in any vector space $V$. Alternatively, we may say that $V$ is closed under linear combination, i.e.

$$
k_1 \boldsymbol{x}_1 + \cdots + k_n \boldsymbol{x}_n \in V
$$

for all $\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n \in V$ and $k_1, \cdots, k_n \in \mathbb{R}$.

For example, below are some valid/invalid vector spaces:

1. $V = \\{(x, y)^\prime: x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space.
2. $V = \\{(x, y)^\prime: x > y\\}$ is not a vector space.
3. $V = \\{(x, y, 0)^\prime: x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space.
4. $V = \\{(x, y, z)^\prime: 2x = y, x \in \mathbb{R}, y \in \mathbb{R}, z \in \mathbb{R}\\}$.

## Subspace

A set $W$ is a `subspace` of a vector space $V$ if $W \subset V$, and $W$ itself is a vector space.

For example, $V = \mathbb{R}^2 = \\{(x, y)^\prime \mid x \in \mathbb{R}, y \in \mathbb{R}\\}$ is a vector space. Let

$$
W_1 = \\{ (x, y)^\prime: y-2x = 0, x, y \in \mathbb{R} \\}
$$

We first check if $W_1$ is a subset of $V$. For any element $\boldsymbol{u} = (u_1, u_2)^\prime \in W_1$ such that $u_2 = 2u_1$, as $\boldsymbol{u}$ is a $2 \times 1$ vector, $\boldsymbol{u} \in V = \mathbb{R}^2$.

Then we need to show that $W_1$ itself is a vector space by showing that $W_1$ is closed under linear combination. For all $\boldsymbol{u}, \boldsymbol{v} \in W_1$, $\boldsymbol{u} = (u_1, u_2)^\prime$ such that $u_2 = 2u_1$, $\boldsymbol{v} = (v_1, v_2)^\prime$ such that $v_2 = 2v_1$, consider

$$
a \boldsymbol{u} + b \boldsymbol{v} = (au_1 + bv_1, au_2 + bv_2)^\prime \in W_1 \quad \forall a, b.
$$

We need to show that $2(au_1 + bv_1) = au_2 + bv_2$, which is obvious. Therefore $W_1$ is a subspace.

Let

$$
W_2 = \\{ (x, y)^\prime: y - 2x + 1 = 0 \\}
$$

Following the same procedure, we can show that $W_2$ is not closed under addition and therefore is not a subspace.

## Span

Let $V$ be an ambient vector space ($\mathbb{R}^2, \mathbb{R}^3, \cdots$). Let $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$[^v-and-s] where $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m \in V$. The `(linear) span` of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$, or $S$, is the set that contains all linear combinations of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. The span is denoted:

$$
\begin{aligned}
    W &= \mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) = span(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) \\\\
    &= \left\\{ \boldsymbol{u} \mid \boldsymbol{u} = \sum\_{i=1}^m k_i \boldsymbol{u}_i, k_i \in \mathbb{R} \right\\}
\end{aligned}
$$

[^v-and-s]: The difference between $V$ and $S$ is that $S$ is a finite set.

We say $W$ is spanned or generated by $S$, or by $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. For example,

$$
\boldsymbol{u}_1 = (1, 0, 0)^\prime, \quad \boldsymbol{u}_2 = (0, 1, 1)^\prime
$$

$W = \mathcal{L}(\boldsymbol{u}_1, \boldsymbol{u}_2)$ contains all possible linear combinations of $\boldsymbol{u}_1$ and $\boldsymbol{u}_2$, such as $(0, 0, 0)^\prime$, $(1, 2, 2)^\prime$, etc.

**A span is a subspace.** Let $W = \mathcal{L}(\boldsymbol{u}\_1, \cdots, \boldsymbol{u}\_m)$ where $\boldsymbol{u}\_i \in V$. For any $\boldsymbol{w} \in W$, $\boldsymbol{w} =\sum\_{i=1}^m c_i \boldsymbol{u}_i \in V$ because $V$ is a vector space. Also for any $\boldsymbol{w}, \boldsymbol{v} \in W$,

$$
a\boldsymbol{w} + b\boldsymbol{v} = a\sum_{i=1}^m c_i \boldsymbol{u}_i + b\sum_{i=1}^m d_i \boldsymbol{u}_i = \sum_{i=1}^m (ac_i + bd_i)\boldsymbol{u}_i,
$$

which is a linear combination of the $\boldsymbol{u}_i$'s in $W$.

### Spanning set

Let $V$ be a vector space (ambient space or a subspace of that). Suppose there exists a set of vectors $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$ in $V$ such that for any $\boldsymbol{u} \in V$, we can express $\boldsymbol{u}$ as a linear combination of $\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m$. Then we say $\\{ \boldsymbol{u}_1, \cdots, \boldsymbol{u}_m \\}$ is a `spanning set` of $V$.

Suppose $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is a spanning set of $V$, then $\mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m) = V$. The LHS is the set of all possible linear combinations of the $\boldsymbol{u}_i$'s. As $V$ is closed under linear combination, LHS $\subset$ RHS. For all $\boldsymbol{v} \in V$, as $S$ is a spanning set, $\boldsymbol{v}$ can be expressed as a linear combination of the $\boldsymbol{u}_i$'s. Since the LHS contains all possible linear combinations of the $\boldsymbol{u}_i$'s, $\boldsymbol{v} \in LHS$, thus RHS $\subset$ LHS.

A vector space is associated with a _finite_ number of vectors. For example, $\mathbb{R}^3$ has a spanning set of {$(1, 0, 0)^\prime$, $(0, 1, 0)^\prime$, $(0, 0, 1)^\prime$}. It's important to know that {{<hl>}}a spanning set is not unique.{{</hl>}} {$(1, 1, 1)^\prime$, $(1, 1, 0)^\prime$, $(0, 1, 1)^\prime$, $(1, 0, 0)^\prime$} is also a valid spanning set for $\mathbb{R}^3$. In fact, there's a infinite number of spanning sets for a vector space.

## Basis

A `basis` is nothing but a linearly independent spanning set. $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is a basis of $V$ if $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m\\}$ is linearly independent, and $\mathcal{L}(\boldsymbol{u}\_1, \cdots, \boldsymbol{u}\_m) = V$, i.e. it's a spanning set of $V$.

In general, a vector space $V$ has infinite bases. But all bases have the same number of vectors, called the `dimension` of $V$. The dimension of $V$ is defined as the maximum number of linearly independent vectors in $V$.

For example, $\mathbb{R}^3$ has a basis $\\{(1, 1, 1)^\prime, (1, 1, 0)^\prime, (0, 1, 1)^\prime\\}$. We can easily show that the set of vectors are LIN. We can also show that $\forall (x, y, z)^\prime \in \mathbb{R}^3$, it can be expressed as a linear combination of the three vectors:

$$
\begin{aligned}
    (x, y, z)^\prime &= a_1 \boldsymbol{u}_1 + a_2 \boldsymbol{u}_2 + a_3 \boldsymbol{u}_3 \\\\
    &= (a_1 + a_2, a_1 + a_2 + a_3, a_1 + a_3)^\prime \\\\
    &\Rightarrow a_3 = y - x, a_1 = z - y + x, a_2 = y - z
\end{aligned}
$$

Given a vector space $W$, e.g. $W = \\{(x, y, z)^\prime \mid y - 2x = 0\\}$, to find a basis of $W$, we first need to find two LIN vectors from $W$. Then, show that

$$
(x, 2x, z)^\prime = x(1, 2, 0)^\prime + z(0, 0, 1)^\prime
$$

to prove $\\{(1, 2, 0)^\prime, (0, 0, 1)^\prime\\}$ is a basis of $W$.

**Theorem:** Every vector in $V$ has a unique representation in terms of a given basis.

Suppose $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is a basis of $V$. Suppose there exists two sets of coefficients $\\{a_1, \cdots, a_n\\}$ and $\\{b_1, \cdots, b_n\\}$ such that $\boldsymbol{x} = \sum a_i\boldsymbol{u}_i$ and $\boldsymbol{x} = \sum b_i \boldsymbol{u}_i$ where $a_i \neq b_i$ for some $i$. If we substract one from the other,

$$
\boldsymbol{0} = \sum_{i=1}^n (a_i - b_i) \boldsymbol{u}_i = \sum_{i=1}^n c_i \boldsymbol{u}_i
$$

As the $\boldsymbol{u}_i$'s are linearly independent, $c_i = 0 \forall i$, i.e. $a_i = b_i \forall i$.

## Dimension

When we say the dimension of a vector space, it's not the same as the number of entries in a vector. For $V = \\{\boldsymbol{0}\\}$, the dimension of $V$ is $dim(V) = 0$. Otherwise, the `dimension` of $V$ is the number of vectors in any basis of $V$.

For $V = \mathcal{L}(\boldsymbol{u}_1, \cdots, \boldsymbol{u}_m)$, $dim(V) = m$.

### Examples

The dimension of $\mathbb{R}^n = n$. A possible basis is

$$
\\{(1, 0, \cdots, 0)^\prime, (0, 1, 0, \cdots, 0)^\prime, \cdots, (0, 0, \cdots, 0, 1)^\prime\\}
$$

For $V = \mathcal{L}(\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3)$ where

$$
\boldsymbol{u}_1 = (1, 1, 1)^\prime, \boldsymbol{u}_2 = (1, 0, -1)^\prime, \boldsymbol{u}_3 = (3, 2, 1)^\prime
$$

We construct linear combinations such that $\sum \alpha_i \boldsymbol{u}_i = 0$:

$$
\begin{cases}
    \alpha_1 + \alpha_2 + 3\alpha_3 = 0 \\\\
    \alpha1 + 2 \alpha_3 = 0 \\\\
    \alpha_1 - \alpha_2 + \alpha_3 = 0
\end{cases} \Rightarrow (\alpha_1, \alpha_2, \alpha_3) = (2\alpha_2, \alpha_2, -\alpha_2)
$$

There exists infinite solutions such as $(2, 1, -1)$, $(4, 2, -2)$ etc., so $\\{\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3\\}$ is linearly dependent, and $dim(V) < 3$.

We can see that $\\{\boldsymbol{u}_1, \boldsymbol{u}_2\\}$ is linearly independent, so they form a basis and $dim(V) = 2$.

The third example is $W = \\{(x, y, z)^\prime \mid 2x - y = 0 \\}$. We have a linear restriction on $x$ and $y$, so the dimension of $W$ is $3-1=2$. Two possible methods are

1. find two LIN vectors in $W$ that form a basis, such as $\\{(0, 0, 1)^\prime, (1, 2, 0)^\prime\\}$. We need to show that these are LIN, and they span $W$.
2. $(x, y, z)^\prime = (x, 2x, z)^\prime = x(1, 2, 0)^\prime + z(0, 0, 1)^\prime$.

The second method is generalizable. Let $W = \\{(x, y, z, w)^\prime \mid x - y + w = 0 \\}$.

$$
(y-w, y, z, w)^\prime = y(1, 1, 0, 0)^\prime + z(0, 0, 1, 0)^\prime + w(-1, 0, 0, 1)^\prime
$$

If we add another constraint of $z = 2w$, then

$$
(y-w, y, 2w, w)^\prime = y(1, 1, 0, 0)^\prime + w(-1, 0, 2, 1)^\prime
$$

The dimension is the dimension of the ambient space $dim(\mathbb{R}^4) = 4$ minus the number of LIN restrictions.

### Facts

1. Every non-null vector space $V$ of finite dimension has a basis.
2. Any two bases of $V$ must have the same number of vectors.
3. If $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ is linearly independent, $\boldsymbol{u}_i \in V$, and the dimension of $V$ is $n$, then $S$ is a basis.
4. If $S = \\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_n\\}$ spans $V$ and $dim(V) = n$, then $S$ is linearly independent and a basis of $V$.
5. If $W$ is a subspace of $V$ and $dim(W) = dim(V)$, then $W = V$.
6. What if $W_1$ and $W_2$ are both subspaces of $V$, and $dim(W_1) = dim(W_2)$? Is $W_1 = W_2$? The answer is **no**.
7. If $dim(V) = n$, and $\\{\boldsymbol{u}_1, \cdots, \boldsymbol{u}_r\\} \subset V$ are linearly independent, then we can find a basis that contains this set as a subset.

## Inner product

The `inner product` or `dot product` is a function that multiplies vectors together into a scalar. The inner product between two vectors is defined as

$$
\langle \boldsymbol{u},\boldsymbol{v} \rangle = \boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{u}^\prime \boldsymbol{v} = \boldsymbol{v}^\prime \boldsymbol{u} = \sum_{i=1}^p u_i v_i
$$

where $\boldsymbol{u} \in V$, $\boldsymbol{v} \in V$ and $V$ is a subspace in $\mathbb{R}^p$. Some properties of the inner product are:

1. $\boldsymbol{u} \cdot \boldsymbol{u} \geq 0$, and $\boldsymbol{u} \cdot \boldsymbol{u} = 0 \Leftrightarrow \boldsymbol{u} = \boldsymbol{0}$.
2. $\boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{v} \cdot \boldsymbol{u}$.
3. $(\boldsymbol{u} + \boldsymbol{v}) \cdot \boldsymbol{w} = \boldsymbol{u} \cdot \boldsymbol{w} + \boldsymbol{v} \cdot \boldsymbol{w}$.
4. $(c\boldsymbol{u}) \cdot \boldsymbol{v} = c(\boldsymbol{u} \cdot \boldsymbol{v})$.

## Norm

The norm of a vector measures the "size" of a vector. Any function that satisfies the following conditions is considered a `norm`:

1. $\\|\boldsymbol{u}\\| \geq 0$.
2. $\\|\boldsymbol{u}\\| = 0$ if $\boldsymbol{u} = \boldsymbol{0}$.
3. $\\|\alpha \boldsymbol{u}\\| = |\alpha| \\|\boldsymbol{u}\\|$.
4. $\\|\boldsymbol{u} + \boldsymbol{v}\\| \leq \\|\boldsymbol{u}\\| + \\|\boldsymbol{v}\\|$.

In this class, we specifically use the `Euclidean norm` ($L_2$ norm), which is defined as

$$
\\|\boldsymbol{u}\\|_2 = \sqrt{\boldsymbol{u} \cdot \boldsymbol{u}} = \sqrt{\sum_{i=1}^n u_i^2}
$$

Another commonly used norm is the $L_1$ norm, which is

$$
\\|\boldsymbol{u}\\|_1 = \sum_{i=1}^n |u_i|
$$

In general, the $L_p$ norm is defined as

$$
\\|\boldsymbol{u}\\|_p = \left(\sum_{i=1}^n |u_i|^p \right)^\frac{1}{p}
$$

and the norms for $0 \leq p \leq 2$ are extensively studied. The `sup-norm` is

$$
\\|\boldsymbol{u}\\|_\infty = \max_{1 \leq i \leq n} |u_i|
$$

and finally the $L_0$ norm is

$$
\\|\boldsymbol{u}\\|_0 = \text{ \# of non-zero elements}
$$

Using the vector $\boldsymbol{u} - (1, 0, 3, -4)^\prime$ as an example, the norms are

$$
\begin{aligned}
    &\\|\boldsymbol{u}\\|_2 = \sqrt{1 + 9 + 16}, &&\\|\boldsymbol{u}\\|_1 = 1 + 3 + 4, \\\\
    &\\|\boldsymbol{u}\\|_\infty = 4, &&\\|\boldsymbol{u}\\|_0 = 3
\end{aligned}
$$

## Distance

The distance between two vectors is denoted $d(\boldsymbol{u}, \boldsymbol{v})$, and has to satisfy the following conditions:

1. $d(\boldsymbol{u}, \boldsymbol{v}) \geq 0$.
2. $d(\boldsymbol{u}, \boldsymbol{v}) = 0$ if and only if $\boldsymbol{u} = \boldsymbol{v}$.
3. $d(\boldsymbol{u}, \boldsymbol{v}) = d(\boldsymbol{v}, \boldsymbol{u})$.
4. Triangular inequality: $d(\boldsymbol{u}, \boldsymbol{w}) \leq d(\boldsymbol{u}, \boldsymbol{v}) + d(\boldsymbol{v}, \boldsymbol{w})$.

The `Euclidean distance` ($L_2$ distance) is the most common choice:

$$
\\|\boldsymbol{u} - \boldsymbol{v}\\|_2 = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
$$

It has a very natural interpretation: imagine a line connecting the two endpoints of the vectors. The $L_2$ distance is the length of this line[^l1-distance].

{{< figure src="distances.png" caption="The red line demonstrates the $L_2$ distance." numbered="true" >}}

[^l1-distance]: For the `Manhattan distaince` or $L_1$ distance, think of it as travelling from $\boldsymbol{u}$ to $\boldsymbol{v}$ but you can only move horizontally or vertically.

### Triangular inequality

Let's see if the $L_2$ norm satisfies the triangular inequality:

$$
\\|\boldsymbol{u} + \boldsymbol{v}\\|_2 \leq \\|\boldsymbol{u}\\|_2 + \\|\boldsymbol{v}\\|_2
$$

As both sides are $\geq 0$, we will prove

$$
\\|\boldsymbol{u} + \boldsymbol{v}\\|_2^2 \leq \left(\\|\boldsymbol{u}\\|_2 + \\|\boldsymbol{v}\\|_2 \right)^2
$$

$$
\begin{gathered}
    LHS = \sum_{i=1}^n (u_i + v_i)^2 = \sum u_i^2 + \sum v_i^2 + 2 \sum u_i v_i
    = \\|\boldsymbol{u}\\|^2 + \\|\boldsymbol{v}\\|^2 + 2\boldsymbol{u}\cdot\boldsymbol{v} \\\\
    RHS = \\|\boldsymbol{u}\\|^2 + \\|\boldsymbol{v}\\|^2 + 2 \\|\boldsymbol{u}\\| \\|\boldsymbol{v}\\|
\end{gathered}
$$

Thus we need to prove $\\|\boldsymbol{u}\\| \\|\boldsymbol{v}\\| \geq \boldsymbol{u} \cdot \boldsymbol{v}$. If $\boldsymbol{u} \cdot \boldsymbol{v} < 0$, the inequality is obvious. When $\boldsymbol{u} \cdot \boldsymbol{v} \geq 0$, we may use the `Cauchy-Schwarz inequality`:

$$
(\boldsymbol{u} \cdot \boldsymbol{v})^2 \leq \\|\boldsymbol{u}\\|^2 \\|\boldsymbol{v}\\|^2
$$

The "high-school" version of this is

$$
(ax + by)^2 \leq (a^2 + b^2)(x^2 + y^2)
$$

This is a special case with $\boldsymbol{u} = (a, b)^\prime$ and $\boldsymbol{v} = (x, y)^\prime$. The equality happens when $bx = ay$, i.e. $\boldsymbol{u} \propto \boldsymbol{v}$.

In terms of real numbers, Cauchy-Schwarz tells us that

$$
\left(\sum u_iv_i \right)^2 \leq \left(\sum u_i^2 \right) \left(\sum v_i^2 \right)
$$

The geometric interpretation of this is as follows.

{{< figure src="law_of_cosine.png" caption="Geometric interpretation of the triangular inequality." numbered="true" >}}

The `law of cosine` tells us that

$$
\begin{aligned}
    \\|\boldsymbol{x} - \boldsymbol{y}\\|^2 &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 - 2\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha) \\\\
    \\|\boldsymbol{x} - \boldsymbol{y}\\|^2 &= (\boldsymbol{x} - \boldsymbol{y})^\prime(\boldsymbol{x} - \boldsymbol{y}) \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 - 2\boldsymbol{x}^\prime \boldsymbol{y} \\\\
    \Rightarrow \boldsymbol{x}^\prime\boldsymbol{y} &= \\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha)
\end{aligned}
$$

We get

$$
\cos(\alpha) = \frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|}
$$

In our example figure above, we can find $\cos(\alpha) = \frac{24-2}{\sqrt{20}\sqrt{37}}$.

As $-1 \leq \cos(\alpha) \leq 1$,

$$
\begin{gathered}
    -1 \leq \frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|} \leq 1 \\\\
    0 \leq \left(\frac{\boldsymbol{x}^\prime \boldsymbol{y}}{\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\|} \right)^2 \leq 1 \\\\
    (\boldsymbol{x}^\prime \boldsymbol{y})^2 \leq \\|\boldsymbol{x}\\|^2 \\|\boldsymbol{y}\\|^2
\end{gathered}
$$

which is the Cauchy-Schwarz inequality. The equality happens when $\alpha = 0$. If $\alpha = 90^\circ$, $\cos(\alpha) = 0$ and $\boldsymbol{x}^\prime \boldsymbol{y} = 0$.

What about $\boldsymbol{x} + \boldsymbol{y}$?

$$
\begin{aligned}
    \\|\boldsymbol{x} + \boldsymbol{y}\\|^2 &= (\boldsymbol{x} + \boldsymbol{y})^\prime (\boldsymbol{x} + \boldsymbol{y}) \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 + 2 \boldsymbol{x}^\prime \boldsymbol{y} \\\\
    &= \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2 + 2\\|\boldsymbol{x}\\| \\|\boldsymbol{y}\\| \cos(\alpha)
\end{aligned}
$$

If $\alpha = 90^\circ$, $\\|\boldsymbol{x} + \boldsymbol{y}\\|^2 = \\|\boldsymbol{x}\\|^2 + \\|\boldsymbol{y}\\|^2$.
